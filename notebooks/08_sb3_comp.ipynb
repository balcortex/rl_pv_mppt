{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import stable_baselines3 as sb3\n",
    "from src.pv_env import PVEnv, PVEnvDiscrete\n",
    "from src.reward import RewardDeltaPower\n",
    "import os\n",
    "\n",
    "PV_PARAMS_PATH = os.path.join(\"parameters\", \"01_pvarray.json\")\n",
    "WEATHER_TRAIN_PATH = os.path.join(\"data\", \"weather_sim.csv\")\n",
    "WEATHER_TEST_PATH = os.path.join(\"data\", \"weather_real.csv\")\n",
    "PVARRAY_CKP_PATH = os.path.join(\"data\", \"01_pvarray_iv.json\")\n",
    "AGENT_CKP_PATH = os.path.join(\"models\", \"02_mppt_ac.tar\")\n",
    "LEARNING_RATE = 0.00001\n",
    "ENTROPY_BETA = 0.001\n",
    "GAMMA = 0.9\n",
    "N_STEPS = 4\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# env = PVEnv.from_file(\n",
    "#     PV_PARAMS_PATH,\n",
    "#     WEATHER_TRAIN_PATH,\n",
    "#     pvarray_ckp_path=PVARRAY_CKP_PATH,\n",
    "#     states=[\"v_norm\", \"i_norm\", \"deg\"],\n",
    "#     reward_fn=RewardDeltaPower(1, 0.9),\n",
    "# )\n",
    "# test_env = PVEnv.from_file(\n",
    "#     PV_PARAMS_PATH,\n",
    "#     WEATHER_TRAIN_PATH,\n",
    "#     pvarray_ckp_path=PVARRAY_CKP_PATH,\n",
    "#     states=[\"v_norm\", \"i_norm\", \"deg\"],\n",
    "#     reward_fn=RewardDeltaPower(1, 0.9),\n",
    "# )\n",
    "\n",
    "env = gym.make('Pendulum-v0')\n",
    "test_env = gym.make('Pendulum-v0')\n",
    "\n",
    "# env = sb3.common.vec_env.VecNormalize()\n",
    "# vec_env = sb3.common.vec_env.dummy_vec_env.DummyVecEnv([lambda: env])\n",
    "# norm_env = sb3.common.vec_env.VecNormalize(vec_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(agent, test_env):\n",
    "    obs = test_env.reset()\n",
    "    for i in range(832):\n",
    "        test_env.render()\n",
    "        action, _ = agent.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = test_env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "    test_env.close()\n",
    "    # test_env.render_vs_true(po=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AssertionError",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-78a3db7e68ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m a2c_agent = sb3.A2C(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mpolicy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"MlpPolicy\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0menv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'lin_7e-4'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cpu'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\venvs\\rl38\\lib\\site-packages\\stable_baselines3\\a2c\\a2c.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, policy, env, learning_rate, n_steps, gamma, gae_lambda, ent_coef, vf_coef, max_grad_norm, rms_prop_eps, use_rms_prop, use_sde, sde_sample_freq, normalize_advantage, tensorboard_log, create_eval_env, policy_kwargs, verbose, seed, device, _init_setup_model)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_init_setup_model\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setup_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\venvs\\rl38\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py\u001b[0m in \u001b[0;36m_setup_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_setup_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setup_lr_schedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_random_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\venvs\\rl38\\lib\\site-packages\\stable_baselines3\\common\\base_class.py\u001b[0m in \u001b[0;36m_setup_lr_schedule\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_setup_lr_schedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[1;34m\"\"\"Transform to callable if needed.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr_schedule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_schedule_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_update_current_progress_remaining\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_timesteps\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\venvs\\rl38\\lib\\site-packages\\stable_baselines3\\common\\utils.py\u001b[0m in \u001b[0;36mget_schedule_fn\u001b[1;34m(value_schedule)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mvalue_schedule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconstant_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue_schedule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue_schedule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mvalue_schedule\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "a2c_agent = sb3.A2C(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=env,\n",
    "    learning_rate=sb3.common.utils.get_schedule_fn()\n",
    "    device='cpu',\n",
    "    ent_coef=0.0,\n",
    "    n_steps=8,\n",
    "    gae_lambda=0.9,\n",
    "    vf_coef=0.4,\n",
    "    use_sde=True,\n",
    "    gamma=0.99,\n",
    "    policy_kwargs=dict(log_std_init=-2, ortho_init=False),\n",
    "    # normalize_advantage=True,\n",
    "    # policy_kwargs={'net_arch':[dict(pi=[128, 128, 128], vf=[128, 128, 128])]}\n",
    "    )\n",
    "ddpg_agent = sb3.DDPG(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=env,\n",
    "    learning_rate=1e-4,\n",
    "    device='cpu',\n",
    "    )\n",
    "ppo_agent = sb3.PPO(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=env,\n",
    "    learning_rate=1e-4,\n",
    "    device='cpu',\n",
    "    )\n",
    "sac_agent = sb3.SAC(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=env,\n",
    "    learning_rate=1e-4,\n",
    "    device='cpu',\n",
    "    )\n",
    "td3_agent = sb3.TD3(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=env,\n",
    "    learning_rate=1e-4,\n",
    "    device='cpu',\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Eval num_timesteps=1033000, episode_reward=-748.41 +/- 373.32\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1034000, episode_reward=-664.63 +/- 312.18\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1035000, episode_reward=-954.52 +/- 68.35\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=1036000, episode_reward=-916.48 +/- 196.36\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=1037000, episode_reward=-847.95 +/- 317.50\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=1038000, episode_reward=-646.45 +/- 296.36\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1039000, episode_reward=-548.70 +/- 270.06\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1040000, episode_reward=-203.87 +/- 109.94\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1041000, episode_reward=-268.34 +/- 182.07\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=1042000, episode_reward=-244.95 +/- 89.71\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Wall time: 25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "a2c_agent.learn(\n",
    "    total_timesteps=10000,\n",
    "    eval_env=test_env,\n",
    "    n_eval_episodes=10,\n",
    "    eval_freq=1000,\n",
    "    reset_num_timesteps=False,\n",
    ")\n",
    "test_agent(a2c_agent, test_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agent(a2c_agent, test_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ActorCriticPolicy(\n",
       "  (features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (shared_net): Sequential()\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=3, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=3, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "a2c_agent.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Eval num_timesteps=3300, episode_reward=-1295.90 +/- 57.43\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3400, episode_reward=-1309.15 +/- 109.91\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=3500, episode_reward=-1277.39 +/- 156.85\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3600, episode_reward=-1294.97 +/- 80.52\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=3700, episode_reward=-1349.82 +/- 162.04\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=3800, episode_reward=-1344.39 +/- 91.88\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=3900, episode_reward=-1288.44 +/- 75.14\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-1246.53 +/- 79.28\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4100, episode_reward=-1268.14 +/- 197.05\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=4200, episode_reward=-1354.15 +/- 31.90\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=4300, episode_reward=-1217.10 +/- 181.59\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4400, episode_reward=-1257.62 +/- 164.17\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=4500, episode_reward=-1274.74 +/- 85.49\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=4600, episode_reward=-1288.70 +/- 57.10\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=4700, episode_reward=-1283.24 +/- 69.12\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=4800, episode_reward=-1295.26 +/- 77.54\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=4900, episode_reward=-1322.31 +/- 60.24\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-1350.85 +/- 74.84\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=5100, episode_reward=-1355.30 +/- 62.01\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=5200, episode_reward=-1361.54 +/- 52.44\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=5300, episode_reward=-1295.00 +/- 78.93\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=5400, episode_reward=-1233.59 +/- 71.72\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=5500, episode_reward=-1328.52 +/- 61.70\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=5600, episode_reward=-1337.39 +/- 75.93\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=5700, episode_reward=-1347.42 +/- 76.69\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=5800, episode_reward=-1364.93 +/- 93.67\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=5900, episode_reward=-1329.66 +/- 65.02\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-1346.01 +/- 65.88\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=6100, episode_reward=-1331.33 +/- 70.38\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=6200, episode_reward=-1339.60 +/- 43.31\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=6300, episode_reward=-1330.78 +/- 35.04\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=6400, episode_reward=-1348.41 +/- 65.24\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=6500, episode_reward=-1336.10 +/- 61.21\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=6600, episode_reward=-1319.99 +/- 57.42\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=6700, episode_reward=-1340.90 +/- 68.48\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=6800, episode_reward=-1327.51 +/- 48.61\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=6900, episode_reward=-1249.18 +/- 59.85\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-1272.30 +/- 43.54\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=7100, episode_reward=-1260.62 +/- 42.42\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=7200, episode_reward=-1284.31 +/- 50.89\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=7300, episode_reward=-1243.39 +/- 62.14\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=7400, episode_reward=-1221.43 +/- 67.43\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=7500, episode_reward=-1232.69 +/- 98.17\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=7600, episode_reward=-1131.65 +/- 194.12\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7700, episode_reward=-1192.04 +/- 183.28\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=7800, episode_reward=-1174.97 +/- 122.90\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=7900, episode_reward=-1153.58 +/- 90.58\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-1186.71 +/- 89.15\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=8100, episode_reward=-1200.60 +/- 65.18\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=8200, episode_reward=-1189.03 +/- 143.18\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=8300, episode_reward=-1133.53 +/- 79.39\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=8400, episode_reward=-1155.35 +/- 74.65\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=8500, episode_reward=-1097.81 +/- 91.77\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8600, episode_reward=-1154.19 +/- 90.78\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=8700, episode_reward=-1153.23 +/- 94.06\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=8800, episode_reward=-1210.48 +/- 88.40\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=8900, episode_reward=-1129.22 +/- 52.29\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-1137.08 +/- 68.62\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=9100, episode_reward=-1108.63 +/- 53.31\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=9200, episode_reward=-1106.13 +/- 55.24\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=9300, episode_reward=-1122.97 +/- 61.20\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=9400, episode_reward=-1162.11 +/- 67.53\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=9500, episode_reward=-1175.39 +/- 54.67\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=9600, episode_reward=-1147.58 +/- 42.38\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=9700, episode_reward=-1155.16 +/- 57.69\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=9800, episode_reward=-1197.87 +/- 41.18\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=9900, episode_reward=-1148.53 +/- 71.15\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-1133.96 +/- 67.77\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=10100, episode_reward=-1112.32 +/- 60.24\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=10200, episode_reward=-1109.12 +/- 59.83\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=10300, episode_reward=-1122.61 +/- 71.08\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=10400, episode_reward=-1156.68 +/- 53.90\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=10500, episode_reward=-1122.79 +/- 58.87\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=10600, episode_reward=-1215.35 +/- 154.08\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=10700, episode_reward=-1119.31 +/- 78.46\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=10800, episode_reward=-1152.59 +/- 64.88\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=10900, episode_reward=-1145.36 +/- 24.39\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-1135.84 +/- 56.84\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=11100, episode_reward=-1138.50 +/- 77.90\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=11200, episode_reward=-1186.81 +/- 119.69\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=11300, episode_reward=-1146.67 +/- 76.99\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=11400, episode_reward=-1169.55 +/- 66.18\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=11500, episode_reward=-1145.86 +/- 52.64\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=11600, episode_reward=-1154.50 +/- 122.29\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=11700, episode_reward=-1188.12 +/- 91.15\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=11800, episode_reward=-1161.99 +/- 97.13\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=11900, episode_reward=-1121.38 +/- 106.29\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-1135.54 +/- 86.37\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=12100, episode_reward=-1154.46 +/- 107.26\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=12200, episode_reward=-1184.23 +/- 86.47\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=12300, episode_reward=-1191.23 +/- 94.62\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=12400, episode_reward=-1198.49 +/- 48.51\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=12500, episode_reward=-1183.38 +/- 98.09\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=12600, episode_reward=-1143.93 +/- 125.81\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=12700, episode_reward=-1186.13 +/- 26.02\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=12800, episode_reward=-1126.42 +/- 95.91\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=12900, episode_reward=-1177.02 +/- 44.13\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-1151.16 +/- 95.78\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=13100, episode_reward=-1113.33 +/- 94.48\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=13200, episode_reward=-1119.16 +/- 110.05\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Wall time: 3min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ddpg_agent.learn(\n",
    "    total_timesteps=10_000,\n",
    "    eval_env=test_env,\n",
    "    n_eval_episodes=10,\n",
    "    eval_freq=100,\n",
    "    reset_num_timesteps=False,\n",
    ")\n",
    "test_agent(ddpg_agent, test_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Eval num_timesteps=100, episode_reward=-1220.50 +/- 343.19\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200, episode_reward=-1170.57 +/- 358.56\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=300, episode_reward=-1213.08 +/- 253.34\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=400, episode_reward=-1121.74 +/- 442.49\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=500, episode_reward=-1177.12 +/- 348.72\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=600, episode_reward=-1098.58 +/- 368.17\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=700, episode_reward=-1420.80 +/- 299.05\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=800, episode_reward=-1278.13 +/- 419.34\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=900, episode_reward=-1315.69 +/- 255.85\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=1000, episode_reward=-1184.39 +/- 325.54\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=1100, episode_reward=-1275.47 +/- 403.05\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=1200, episode_reward=-1133.01 +/- 252.33\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=1300, episode_reward=-1353.64 +/- 181.85\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=1400, episode_reward=-1021.28 +/- 365.49\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1500, episode_reward=-1434.72 +/- 211.17\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=1600, episode_reward=-1186.54 +/- 237.88\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=1700, episode_reward=-1256.96 +/- 339.99\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=1800, episode_reward=-1341.57 +/- 269.74\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=1900, episode_reward=-1280.73 +/- 385.20\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=2000, episode_reward=-1209.87 +/- 280.69\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=2100, episode_reward=-1285.08 +/- 320.21\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=2200, episode_reward=-1133.51 +/- 371.94\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=2300, episode_reward=-1217.98 +/- 401.75\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=2400, episode_reward=-1466.46 +/- 306.52\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=2500, episode_reward=-1264.02 +/- 335.77\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=2600, episode_reward=-1067.20 +/- 314.08\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=2700, episode_reward=-1317.63 +/- 408.83\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=2800, episode_reward=-1213.45 +/- 347.20\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=2900, episode_reward=-1418.38 +/- 220.61\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-1297.96 +/- 237.32\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=3100, episode_reward=-1287.52 +/- 377.61\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=3200, episode_reward=-1184.52 +/- 361.07\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=3300, episode_reward=-966.63 +/- 295.73\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3400, episode_reward=-1311.36 +/- 340.61\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=3500, episode_reward=-1128.86 +/- 314.68\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=3600, episode_reward=-1309.30 +/- 306.92\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=3700, episode_reward=-1303.29 +/- 299.36\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=3800, episode_reward=-1412.83 +/- 345.56\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=3900, episode_reward=-1246.46 +/- 367.45\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-1262.03 +/- 368.51\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=4100, episode_reward=-1070.79 +/- 241.20\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=4200, episode_reward=-1101.51 +/- 311.02\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=4300, episode_reward=-1113.60 +/- 208.52\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=4400, episode_reward=-1276.45 +/- 319.29\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=4500, episode_reward=-1151.78 +/- 224.26\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=4600, episode_reward=-1065.70 +/- 267.23\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=4700, episode_reward=-1009.78 +/- 185.52\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=4800, episode_reward=-1257.44 +/- 348.29\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=4900, episode_reward=-1245.02 +/- 284.96\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-1221.80 +/- 395.95\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=5100, episode_reward=-969.54 +/- 129.95\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=5200, episode_reward=-1187.46 +/- 301.26\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=5300, episode_reward=-1270.80 +/- 365.21\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=5400, episode_reward=-1017.07 +/- 202.40\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=5500, episode_reward=-1130.64 +/- 304.39\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=5600, episode_reward=-1051.19 +/- 193.26\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=5700, episode_reward=-1219.65 +/- 234.91\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=5800, episode_reward=-1085.27 +/- 152.15\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=5900, episode_reward=-1146.56 +/- 329.27\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-940.56 +/- 100.72\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6100, episode_reward=-1072.38 +/- 280.91\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=6200, episode_reward=-1334.68 +/- 344.82\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=6300, episode_reward=-1165.83 +/- 279.88\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=6400, episode_reward=-1292.81 +/- 381.70\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=6500, episode_reward=-1142.31 +/- 339.77\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=6600, episode_reward=-1234.14 +/- 362.58\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=6700, episode_reward=-1235.89 +/- 281.60\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=6800, episode_reward=-1180.95 +/- 384.79\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=6900, episode_reward=-1307.87 +/- 425.09\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-1442.66 +/- 335.65\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=7100, episode_reward=-1249.85 +/- 214.28\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=7200, episode_reward=-1380.57 +/- 347.77\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=7300, episode_reward=-1362.65 +/- 101.15\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=7400, episode_reward=-953.67 +/- 207.45\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=7500, episode_reward=-997.39 +/- 305.08\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=7600, episode_reward=-1125.48 +/- 337.18\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=7700, episode_reward=-1293.27 +/- 317.96\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=7800, episode_reward=-1172.13 +/- 243.55\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=7900, episode_reward=-1177.61 +/- 404.41\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-1133.26 +/- 267.13\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=8100, episode_reward=-1326.40 +/- 338.14\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=8200, episode_reward=-1447.80 +/- 358.68\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=8300, episode_reward=-1228.53 +/- 237.45\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=8400, episode_reward=-1180.52 +/- 244.36\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=8500, episode_reward=-1187.24 +/- 308.53\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=8600, episode_reward=-1145.99 +/- 170.35\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=8700, episode_reward=-1182.86 +/- 314.63\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=8800, episode_reward=-1211.06 +/- 304.30\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=8900, episode_reward=-1338.72 +/- 419.31\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-1279.05 +/- 267.96\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=9100, episode_reward=-1052.08 +/- 195.10\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=9200, episode_reward=-1288.05 +/- 337.74\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=9300, episode_reward=-1155.88 +/- 311.57\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=9400, episode_reward=-1189.92 +/- 373.21\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=9500, episode_reward=-1209.00 +/- 370.77\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=9600, episode_reward=-1182.28 +/- 417.07\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=9700, episode_reward=-1438.36 +/- 338.68\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=9800, episode_reward=-1225.09 +/- 310.83\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=9900, episode_reward=-1342.32 +/- 342.82\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-1037.35 +/- 169.85\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=10100, episode_reward=-1277.84 +/- 343.61\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=10200, episode_reward=-1197.23 +/- 341.45\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Wall time: 1min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ppo_agent.learn(\n",
    "    total_timesteps=10_000,\n",
    "    eval_env=test_env,\n",
    "    n_eval_episodes=10,\n",
    "    eval_freq=100,\n",
    "    reset_num_timesteps=False,\n",
    ")\n",
    "test_agent(ppo_agent, test_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Eval num_timesteps=100, episode_reward=-1285.18 +/- 279.17\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200, episode_reward=-1125.35 +/- 122.73\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=300, episode_reward=-1287.15 +/- 163.55\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=400, episode_reward=-1429.65 +/- 125.88\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=500, episode_reward=-1441.35 +/- 160.52\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=600, episode_reward=-1616.27 +/- 108.92\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=700, episode_reward=-1748.70 +/- 69.85\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=800, episode_reward=-1773.50 +/- 103.19\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=900, episode_reward=-1785.86 +/- 93.70\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=1000, episode_reward=-1801.09 +/- 95.03\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=1100, episode_reward=-1793.29 +/- 103.95\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=1200, episode_reward=-1782.50 +/- 90.94\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=1300, episode_reward=-1794.63 +/- 97.53\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=1400, episode_reward=-1739.09 +/- 157.73\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=1500, episode_reward=-1735.63 +/- 147.91\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=1600, episode_reward=-1677.80 +/- 133.79\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=1700, episode_reward=-1693.01 +/- 140.12\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=1800, episode_reward=-1637.65 +/- 148.68\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=1900, episode_reward=-1622.27 +/- 92.13\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=2000, episode_reward=-1524.43 +/- 175.43\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=2100, episode_reward=-1423.27 +/- 93.76\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=2200, episode_reward=-1366.69 +/- 125.40\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=2300, episode_reward=-1404.61 +/- 101.23\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=2400, episode_reward=-1338.93 +/- 131.45\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=2500, episode_reward=-1379.37 +/- 139.80\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=2600, episode_reward=-1346.52 +/- 109.35\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=2700, episode_reward=-1248.66 +/- 113.04\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=2800, episode_reward=-1244.72 +/- 106.03\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=2900, episode_reward=-1247.09 +/- 112.59\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-1140.79 +/- 148.59\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=3100, episode_reward=-1222.72 +/- 104.22\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=3200, episode_reward=-1221.74 +/- 105.30\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=3300, episode_reward=-1261.61 +/- 97.54\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=3400, episode_reward=-1323.43 +/- 121.51\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=3500, episode_reward=-1217.04 +/- 98.51\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=3600, episode_reward=-1155.76 +/- 54.92\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=3700, episode_reward=-1212.24 +/- 112.32\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=3800, episode_reward=-1272.28 +/- 66.89\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=3900, episode_reward=-1270.26 +/- 74.52\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-1229.95 +/- 64.24\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=4100, episode_reward=-1157.86 +/- 46.31\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=4200, episode_reward=-1195.38 +/- 74.80\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=4300, episode_reward=-1112.54 +/- 88.21\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4400, episode_reward=-1121.29 +/- 95.37\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=4500, episode_reward=-1115.21 +/- 90.62\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=4600, episode_reward=-1114.57 +/- 91.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=4700, episode_reward=-1159.00 +/- 83.73\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=4800, episode_reward=-1206.65 +/- 128.75\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=4900, episode_reward=-1196.92 +/- 73.78\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-1170.24 +/- 91.97\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=5100, episode_reward=-1118.48 +/- 65.68\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=5200, episode_reward=-1098.16 +/- 73.23\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5300, episode_reward=-1181.89 +/- 63.85\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=5400, episode_reward=-1283.84 +/- 57.63\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=5500, episode_reward=-1250.57 +/- 77.32\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=5600, episode_reward=-1261.89 +/- 38.59\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=5700, episode_reward=-1248.84 +/- 90.55\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=5800, episode_reward=-1206.43 +/- 101.29\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=5900, episode_reward=-1203.68 +/- 88.95\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-1233.08 +/- 89.60\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=6100, episode_reward=-1237.97 +/- 96.63\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=6200, episode_reward=-1255.18 +/- 80.32\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=6300, episode_reward=-1266.55 +/- 66.35\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=6400, episode_reward=-1278.30 +/- 79.93\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=6500, episode_reward=-1267.89 +/- 89.88\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=6600, episode_reward=-1259.57 +/- 83.74\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=6700, episode_reward=-1263.13 +/- 56.59\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=6800, episode_reward=-1280.29 +/- 62.04\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=6900, episode_reward=-1294.53 +/- 56.38\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-1242.19 +/- 96.44\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=7100, episode_reward=-1315.39 +/- 55.24\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=7200, episode_reward=-1322.36 +/- 61.35\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=7300, episode_reward=-1269.55 +/- 55.03\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=7400, episode_reward=-1193.83 +/- 67.80\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=7500, episode_reward=-1239.01 +/- 79.67\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=7600, episode_reward=-1174.00 +/- 51.50\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=7700, episode_reward=-1197.43 +/- 122.88\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=7800, episode_reward=-1258.76 +/- 98.84\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=7900, episode_reward=-1258.40 +/- 59.30\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-1153.43 +/- 76.30\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=8100, episode_reward=-1260.66 +/- 68.29\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=8200, episode_reward=-1194.43 +/- 37.26\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=8300, episode_reward=-1137.43 +/- 55.38\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=8400, episode_reward=-1176.54 +/- 42.21\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=8500, episode_reward=-1194.88 +/- 32.02\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=8600, episode_reward=-1110.20 +/- 113.49\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=8700, episode_reward=-1110.82 +/- 64.96\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=8800, episode_reward=-1150.05 +/- 37.72\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=8900, episode_reward=-1139.09 +/- 30.14\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-1076.15 +/- 35.15\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9100, episode_reward=-1107.09 +/- 50.25\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=9200, episode_reward=-1108.07 +/- 65.14\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=9300, episode_reward=-1151.13 +/- 30.99\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=9400, episode_reward=-1108.55 +/- 39.25\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=9500, episode_reward=-1121.59 +/- 88.81\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=9600, episode_reward=-1140.33 +/- 60.54\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=9700, episode_reward=-1167.60 +/- 11.59\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=9800, episode_reward=-1103.77 +/- 86.24\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=9900, episode_reward=-1150.35 +/- 45.22\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-1138.80 +/- 36.25\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Wall time: 6min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sac_agent.learn(\n",
    "    total_timesteps=10_000,\n",
    "    eval_env=test_env,\n",
    "    n_eval_episodes=10,\n",
    "    eval_freq=100,\n",
    "    reset_num_timesteps=False,\n",
    ")\n",
    "test_agent(sac_agent, test_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Eval num_timesteps=100, episode_reward=-1239.03 +/- 153.52\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200, episode_reward=-1277.18 +/- 266.23\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=300, episode_reward=-1848.85 +/- 74.67\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=400, episode_reward=-1797.95 +/- 91.26\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=500, episode_reward=-1451.33 +/- 131.93\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=600, episode_reward=-1474.77 +/- 155.30\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=700, episode_reward=-1432.77 +/- 124.87\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=800, episode_reward=-1461.72 +/- 158.48\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=900, episode_reward=-959.18 +/- 228.22\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-1064.03 +/- 211.57\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=1100, episode_reward=-1156.45 +/- 39.48\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=1200, episode_reward=-1077.90 +/- 150.17\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=1300, episode_reward=-1336.93 +/- 181.18\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=1400, episode_reward=-1418.97 +/- 142.53\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=1500, episode_reward=-1145.00 +/- 228.16\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=1600, episode_reward=-1172.06 +/- 206.68\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=1700, episode_reward=-1341.62 +/- 142.87\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=1800, episode_reward=-1363.51 +/- 150.16\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=1900, episode_reward=-1240.08 +/- 159.95\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=2000, episode_reward=-1238.69 +/- 130.08\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=2100, episode_reward=-1264.08 +/- 204.04\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=2200, episode_reward=-1282.19 +/- 155.58\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=2300, episode_reward=-1272.52 +/- 214.34\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=2400, episode_reward=-1135.39 +/- 257.23\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=2500, episode_reward=-1097.79 +/- 218.34\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=2600, episode_reward=-1249.56 +/- 196.90\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=2700, episode_reward=-1245.40 +/- 162.89\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=2800, episode_reward=-1201.72 +/- 176.79\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=2900, episode_reward=-1175.78 +/- 176.42\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-1194.05 +/- 191.94\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=3100, episode_reward=-1247.00 +/- 248.76\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=3200, episode_reward=-1146.41 +/- 228.86\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=3300, episode_reward=-1194.36 +/- 147.71\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=3400, episode_reward=-1202.27 +/- 135.08\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=3500, episode_reward=-1160.02 +/- 168.84\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=3600, episode_reward=-1106.90 +/- 147.23\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=3700, episode_reward=-1067.72 +/- 131.50\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=3800, episode_reward=-1213.87 +/- 176.21\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=3900, episode_reward=-1201.29 +/- 162.33\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-1177.25 +/- 160.92\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=4100, episode_reward=-1184.63 +/- 161.65\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=4200, episode_reward=-1313.03 +/- 196.52\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=4300, episode_reward=-1143.92 +/- 161.68\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=4400, episode_reward=-1224.05 +/- 199.17\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=4500, episode_reward=-1242.08 +/- 140.60\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=4600, episode_reward=-1241.78 +/- 127.70\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=4700, episode_reward=-1296.64 +/- 205.66\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=4800, episode_reward=-1211.70 +/- 169.82\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=4900, episode_reward=-1327.80 +/- 108.32\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-1262.89 +/- 128.39\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=5100, episode_reward=-1229.90 +/- 75.38\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=5200, episode_reward=-1209.50 +/- 121.75\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=5300, episode_reward=-1290.57 +/- 137.70\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=5400, episode_reward=-1224.88 +/- 132.80\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=5500, episode_reward=-1292.33 +/- 87.40\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=5600, episode_reward=-1268.86 +/- 100.47\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=5700, episode_reward=-1241.49 +/- 50.31\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=5800, episode_reward=-1284.78 +/- 87.69\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=5900, episode_reward=-1260.19 +/- 138.78\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-1232.19 +/- 114.82\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=6100, episode_reward=-1281.57 +/- 122.53\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=6200, episode_reward=-1240.03 +/- 123.89\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=6300, episode_reward=-1241.66 +/- 119.74\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=6400, episode_reward=-1279.48 +/- 134.13\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=6500, episode_reward=-1188.53 +/- 218.50\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=6600, episode_reward=-1168.02 +/- 175.98\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=6700, episode_reward=-1138.99 +/- 144.53\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=6800, episode_reward=-1228.83 +/- 100.28\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=6900, episode_reward=-1117.21 +/- 163.57\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-1078.76 +/- 63.37\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=7100, episode_reward=-1140.35 +/- 150.50\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=7200, episode_reward=-1020.93 +/- 128.33\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=7300, episode_reward=-1009.44 +/- 106.46\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=7400, episode_reward=-1086.82 +/- 150.58\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=7500, episode_reward=-932.01 +/- 122.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7600, episode_reward=-984.07 +/- 200.42\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=7700, episode_reward=-1009.77 +/- 210.18\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=7800, episode_reward=-851.22 +/- 120.63\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7900, episode_reward=-972.47 +/- 269.04\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-858.81 +/- 97.27\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=8100, episode_reward=-982.41 +/- 186.98\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=8200, episode_reward=-993.23 +/- 186.83\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=8300, episode_reward=-994.63 +/- 182.11\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=8400, episode_reward=-999.87 +/- 178.38\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=8500, episode_reward=-1090.65 +/- 152.20\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=8600, episode_reward=-1047.42 +/- 175.21\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=8700, episode_reward=-1077.63 +/- 89.98\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=8800, episode_reward=-1035.09 +/- 53.43\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=8900, episode_reward=-1160.07 +/- 59.20\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-1118.96 +/- 135.30\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=9100, episode_reward=-1199.44 +/- 111.89\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=9200, episode_reward=-1222.45 +/- 42.57\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=9300, episode_reward=-1183.24 +/- 200.27\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=9400, episode_reward=-1141.38 +/- 243.48\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=9500, episode_reward=-1170.74 +/- 133.68\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=9600, episode_reward=-1196.49 +/- 158.46\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=9700, episode_reward=-1086.04 +/- 179.03\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=9800, episode_reward=-1061.74 +/- 184.66\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=9900, episode_reward=-1167.26 +/- 190.85\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-1018.71 +/- 48.68\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Wall time: 3min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "td3_agent.learn(\n",
    "    total_timesteps=10_000,\n",
    "    eval_env=test_env,\n",
    "    n_eval_episodes=10,\n",
    "    eval_freq=100,\n",
    "    reset_num_timesteps=False,\n",
    ")\n",
    "test_agent(td3_agent, test_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}